# 3. Artificial Neural Network PART 2

- ##### XOR by PyTorch

  ![image-20250204110630957](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204110630957.png)

  - `np.reshape` - > 행렬을 바꾸는거, 즉 모양을 바꾸는 함수
  - `torch.tensor` -> 차원이 굉장히 큰 `Array`, `순환 Array`, `dtype = torch.float` -> 타입을 `float`로 변경

  ![image-20250204112352943](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204112352943.png)

  - 학습하는 `Function`은 `옵티마이저 (optimizer)` 라고 함
    - 머신 러닝에서 비용 함수를 최소화 하거나 목적 함수를 최대화 하기 위해 모델의 여러 파라미터들을 업데이트하는 데 사용하는 알고리즘

  ![image-20250204113737778](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204113737778.png)

  - `logits` = `(hypothesis > 0.5).float` 은 위에서 `hypothesis`의 값이 `1`아니면 `0`이기 때문에 앞서 `true` or `false`의 값만 나오게 됨
    - 그 후 나온 값에 따라 `1.0`, `0.0`으로 `float` 형으로 처리해줌
    - 출력 값, `argmax` 값 이라고도 함
  - 역전파는 신경망의 각 노드가 가지고 있는 가중치(Weight)와 편향(Bias)을 학습시키기 위한 알고리즘으로, 딥러닝에 있어서 가장 핵심적인 부분
    - 목표(Target)와 모델의 예측 결과(Output)가 얼마나 차이가 나는지 확인하고 그 오차를 바탕으로 가중치와 편향을 뒤에서부터 앞으로 갱신해가는 것을 의미



- `Wide ANN`

  ![image-20250204124538426](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204124538426.png)

  - `Widening`은 선을 하나씩 늘리는 효과



- ##### Shallow ANN

  ![image-20250204124843795](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204124843795.png)

  - `Single-layer Perceptron`은 `input`에서 바로 `output` 연결 되는 형태
    - `은닉층 ( Hidden Layer )`가 없음
  - `Single-layer Perceptron`은 학습 속도는 빠르지만 문제를 풀지는 못함



- `Deep ANN`

  ![image-20250204131213621](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204131213621.png)

  - `Hidden layer`를 2개로 변경해서 학습 시간이 오래 걸림
  - 오랬동안 학습하여 우리가 원하는 결과를 도출함
  - `Deeping` 선을 구부리는 효과
  - 이전의 `output`값 한번 통과하여 `시그모이드`를 씌운 값이여서 `1`과 `0`의 값을 가지는데 그걸 다시 입력을 받기 때문에 조금씩 구부려나가는 효과로 이해하면 됨
    - 수식적으로는 커널 펑션을 통해서 다른 위치로 옮겨가는 것



- ##### Deeper ANN

  ![image-20250204131610148](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204131610148.png)

  - 기울기 소실 문제때문에 학습이 되질 않음



- ##### Vanishing Gradient ( 기울기 소실 문제 ) 

  ![image-20250204133158931](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204133158931.png)

  - 위 그림을 보면 오른쪽에서 오류를 전파하는데 깊게 들어가보니까 오류가 희미해져서 아래층에는 전달이 안됨
  -  `f(x)`의 값은 `시그모이드`를 해주기 때문에 `0`과`1`사이의 값이다.
    - 그런데 이것을 계속 곱해주니까 `0`이 되버림 너무 작아짐 1보다 작은 값을 곱해서
    - 따라서, 기울기가 없어지는거임 
    - 깊게 쌓아지기 때문에 `weight`가 업데이트가 안됨



- ##### Sigmoid to ReLU

  - 기울기 소실의 문제를 해결시킨 방법

  - 문제가 `Sigmoid`떄문이라고 결론을 내림

    - 그래서 `Logistic Sigmoid` 인 엑티베이션 함수만 `ReLU` 함수로 바꿈

  - ##### ReLU (Rectified Linear Unit)

    - `0`을 기점으로 해서 그 밑으론 싹 다 `0`
    - `0`보다 큰 것은 `y = x` 형태의 그래프를 지님
      - `y = x`를 미분하면 `1`이 됨
      - `1`의 형태로 전달됨, 즉 , 모양 그대로
      - 그러면 계속 곱해나아가도 기울기가 소실이 안된다고 추측을 할 수 있음 

  ![image-20250204142820854](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204142820854.png)

  - 마지막은 `Classfication` 문제니까 `0`과 `1`사이의 값을 나와야 되기 때문에 마지막은 `Sigmoid` 를 유지함

  ![image-20250204142915042](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204142915042.png)

  - 10,000 번을 돌려도 문제가 해결되지 않았는데 `ReLU`로 바꾸고 3,000 번만 실행해도 문제가 해결 되는 것을 볼 수 있음

  ![image-20250204143722104](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204143722104.png)

  - `Leaky ReLU`
    - 기울기를 줘서 조금씩 감소하도록 주는 함수
  - `Randomized Leaky ReLU`
    - 랜덤하게 변형되는 함수
  - 간단한 문제는 `ReLU`를 쓰면 해결 됨
    - 상황에 따라 위 두 함수를 활용하여 해결



- ##### Fitting ( 적합 )

  ![image-20250204144311759](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204144311759.png)

  - 1번과 같은 것을 `Underfitting` 이라함
  - 우리는 2번과 같은 좋은 커브를 가지고 있는 직선을 원하는데 그거에 못미치는 직선인 1번 같은 직선을 `Underfitting`이라 함
    - 또는 `High bias`라고 함
    - 아직 훈련학습이 덜 된 상태
    - 학습을 더 하거나 `Learning rate`를 잘 조절해야함 
  - 3번과 같은 것을 `Overfitting`이라 함
    - 학습 결과가 훈련 데이터에 너무 맞춰진 것
    - 훈련 데이터에는 성능이 너무 좋은데 새로운 데이터가 들어오면 거의 설명을 못함
    - 훈련 데이터가 너무 적은 경우 이러한 결과가 발생됨 맞춤식으로 학습이 됨
    - 따라서, 훈련 데이터를 늘려야함
    - 또는, `features`를 디자인했는데 너무 많이 했을 수도 있음 그래서 `features`를 줄일 생각도 해보야함
    - 그대로 안되면 `Regularization` 이라는 방법을 해봐야함



- ##### Regularization ( 정규화, 일반화 )

  - `Cost Function` 값이 작아지는 방향으로 학습하는 과정에서 특정 가중치가 너무 커져서 일반화 성능이 떨어지는 것을 방지하기 위한 방법

  ![image-20250204144645900](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204144645900.png)

  - 방법에는 `4`가지 방법이 있음

  - 모델한테 수학적으로 입력을 해주는 방법은 `L1, L2 regularization 뿐임`

    - `Loss Function`을 업데이트 하는 방법임

  - `L1` -> 절대값을 다 더해준 것

    - 그 값까지 `Loss Function`에 더해져서 우리가 최솟값을 구하는 것
    - `weight`가 무진장 커지는 것을 방지하는 것
    - `Feature selection`
      - 쓸데없는 `Feature`가 `0`으로 가도 전혀 값에는 변함이 없음
      - 가지고 있는 특성 중에서 훈련에 가장 유용한 특성을 선택하는 것
      - 모델의 분류 정확도를 향상시키기 위해, 원본 데이터에서 가장 좋은 성능을 보여불 수 있는 데이터의 부분집합(Subset)을 찾아내는 방법

  - `L2` ->  각각의 원소를 제곱해준 것

    - 전체적으로 `값(weight)`을 줄이는 방향으로 감
    - `Weight decay`

  - ##### Early Stopping

    - `Dev set`에서 성능이 더 이상 증가하지 않을 때 지정 횟수보다 학습을 일찍 끝내는 것

  - ##### Dropout

    - 학습 과정 중에 지정된 비율로 임의의 연결을 끊음으로써 일반화 성능을 개선하는 방법
    - 노이즈 데이터, 즉, 예상치 못한 데이터가 들어왔을 때 적응하는 능력이 떨어진 데이터가 있을 때 일부러 끊어버림
    - 적은 데이터를 받아도 판단할 수 있는 능력을 키우는 것
    - 입력이 예상하지 못한 노이즈가 섞여 있어도 그것에 잘 대응하도록 `weight`를 조정하는 것을 `Dropout`

  ![image-20250204152208820](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204152208820.png)

​	

- ##### Residual Connection ( 추가 연결 )

  - 가중치층을 우회하여 상위 층으로 직접 연결하는 것
  - 추상화 정도를 적절히 섞어주는 효과 - > 앙상블 효과를 통해 성능 개선
    - **앙상블**
      - 여러 가지를 섞어서 효과를 증대시키는 것

  ![image-20250204154706312](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204154706312.png)

​	![image-20250204155410532](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204155410532.png)

- **model.train()**
  - 학습 모드 세팅
  - `Dropout` 이나 `역전파`를 수행할 수 있음



- ##### Test 함수

  ![image-20250204160910390](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204160910390.png)

  ![image-20250204161309034](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204161309034.png)

  ![image-20250204161350949](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250204161350949.png)