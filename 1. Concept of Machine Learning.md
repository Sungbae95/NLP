# 1. Concept of Machine Learning

- ##### LInear Hypothesis

  - 직선을 찾는 것을 가정
  - `H(x) = Wx + b`

- ##### Multi-variable

  - `H(x1, x2) = w1x1 + w2x2 + b`
    - 위에는 `w`와 `b`만 찾는 문제면 이 방정식은 `w1`, `w2`, `b`를 찾는 문제임
    - 즉, 늘어날수록 확대만 되면 됨
    
  - 위의 방정식을 `Matrix representation` 으로 바꾸면
  
  
  
  ![image-20250203105122460](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203105122460.png)
  
  - 이와 같은 `Matrix`로 표현할 수 있음
  - 따라서, `H(x) = Wx + b` 로 표현할 수 있음
    - `w` = weight , `x` = vector, `b` = bias
  
  - 방정식에 `b`를 추가하고 싶으면 위와 같이 추가하면 됨
  
  ![image-20250203105630064](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203105630064.png)
  
  - 원래는 위 그림과 같은 표현 식이 맞음
  - 그러나, 귀찮아서 보통 `H(x) = Wx ` or `H(x) = Wx + b` 둘 중 하나로 표현
  
- ##### Which hypothesis is better?

  - 오차가 가장 적은 `w`와 `b`를 찾아야함
  - `H(x) - y` -> 에서 `H(x) = Wx + b` 이고 `y`는 `error` 
  - 처음에는 아무 값을 가진 `w`와 `b`를 줌
    - 거기에서 원래의 정답을 빼면 오차 

  ![image-20250203110210513](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203110210513.png)

  - 위의 그림에서 직선을 보면 `x`의 `1`의 값을 넣었을 때 `1.xxx`의 값을 가지면서 직선을 그리게 됨
  - 허나, 원래의 정답은 `1`을 넣었을 때 `1`의 값을 가지게 됨
  - 즉, 정답의 값과 `H(x)`의 차이가 `error`인 것
  - 우리는 저 `error`가 가장 적은 것을 찾아야함

- ##### Cost Function (비용 함수)

  - 가장 좋은 방법은 `제곱`을 하는 것
  - 그 다음에 평균을 구하면 됨

  ![image-20250203111250918](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203111250918.png)

  - 위 그림과 같이 각 값의 `제곱`을 해준 후 `3`까지의 값을 구했으니, `3`으로 나누어 평균을 구하면 됨
  - 위와 같은 방법을 `MSE` = `Mean Squared Error`, 오차를 제곱한 값의 평균, `평균 제곱 오차` 라고 함

  ![image-20250203111528074](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203111528074.png)

  - 이러한 것을 `Cost Function` 즉, `비용 함수`라고 하는데 지금 내가 찾은 직선이 정답으로 가려면 얼마나 많은 비용이 필요로 하는 함수
  - 비용을 최소로 하는 직선이 가장 좋은 직선
  - 여기서 `MSE`가 대표적으로 쓰이는 것 중 하나
    - 직선을 찾을 때 가장 많이 사용되는 비용 함수임
  - 직선을 찾는 문제를 `Regression problem`
  - 영화의 평점 같은 것을 맞추는 곳에 쓰임
  - 입력을 하면 정답이 바로 나오는 곳에 사용

  ![image-20250203112151360](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203112151360.png)



- ##### What cost(W) looks like?

  ![image-20250203130244305](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203130244305.png)

  - 기울기가 `-`일 때는 `w`가 `+`방향으로 움직이고 기울기가 `+`면 `w`가 `-`방향으로 움직이면 됨
  - 기울기가 완만하면 천천히 움직이고 완만하지 않으면 급하게 움직
  - 그래서 처음에는 `w`와 `b` 의 값을 랜덤하게 주고 해당 위치에서 기울기를 구하고 기울기의 값에 따라 업데이트를 해줌

- ##### How to Minimize Cost?

  ![image-20250203130637430](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203130637430.png)

  - 이런 과정을 `Gradient Decent` = `경사 하강법` 이라 함

  ![image-20250203131033607](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203131033607.png)

  - 여기서 `a = 알파`는 `learning rate` 라고 하는데 이 값은 경험에 의해 주면 됨
  - 너무 값을 작게 주면 너무 오래걸림
    - 그렇다고 너무 크게 주면 안됨



- ##### Convex Function

  - 무조건 값은 찾을 수 있음

  - 비용 함수를 잘못 만들면 위와 같은 그림으로 만들어지지 않음

  - ##### Local Minimum( Local Minima )

    - 목표가 `cost`가 최소가 되길 원하는데 자칫 잘못하여 원하는 목표가 아닌 곳에서 `cost` 값이 제일 작다고 판별하는 경우

  - ##### Global Minimum

    - 최적의 `cost`값

  - 머신러닝에서는 `Global Minimum`을 찾아야 하는 것이고 `Local Minimum`을 피해야 하는 것

  - `Convex Function`은 무조건 `Global Minimum`을 찾을 수 있고 그러므로 `Cost Function` 설계가 잘되야함



- ##### Regression to Classification

  ![image-20250203133027866](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203133027866.png)

  - `Regression` 가지고도 `Classfication`을 문제를 풀 순 있지만 위의 예시와 같이 직선에 위에 있으면 `pass` 아래에 있으면 `fail`이라고 가정했을 때 `0.5`라는 값만 넘어도 사실 `pass`인데 `Regresssion`으로 했을 때 직선의 아래에 있기 때문에 `fail`의 값을 가지게 되는 문제를 가지게 됨
  - 이러한 문제를 해결하기 위해 `Logistic Hypothesis`를 사용



- ##### Logistic Hypothesis

  - `true`와 `false`를 찾을 때 사용

  ![image-20250203133831628](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203133831628.png)

  - 위와 같은 그래프를 `sigmoid` 함수라고 함
  - `-`로 계속 나아가도 `0`을 넘지 않고 `+`로 계속 나아가도 `1`을 넘지 않음

  ![image-20250203134043797](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203134043797.png)

  - 위의 그림과 같은 방법으로 `Classfication`문제를 품
  - `Regression`에만 `MSE`를 사용함
  - `Logisctic Hypothesis`에 사용하는 새로운 `Cost Function`을 하나 배움
    - `Cost Function`은 오답일 때는 `cost`값이 엄청나게 크고 정답일 때는 `cost`값이 작으면 됨
    - 이렇게만 디자인 하면 됨
  - 위의 예시를 보면 `y = 1`일 때 `log(1) = 0` 의 값을 가지게 됨, 그리고 `H(x) = 0`이라고 가정했을 때는 `+무한대`의 값을 가지게 됨
  - 정답이 `0`일 때도 `0`과 `무한대`의 값을 가지게됨
  - 따라서, 우리가 찾는 정답일 때는 `0`의 값을 가지고 (가장 작은 값), 오답일 때는 가장 큰 값(+무한대)를 가지게 되는 `Cost Function`을 가짐



- ##### Binary Classfication

  - `1`이냐 `0`이냐 둘 중에 하나를 구분하는 것

  ![image-20250203135505391](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203135505391.png)

  - `y`가 `1`일 때랑 `0`일 때랑 구분해서 함수를 쓰면 미분하기 번거로우니
  - 위 그림의 마지막 함수처럼 표현을 함
    - 그러면 해당 값은 `y`가 `0`일 때는 앞의 값은 `y`가 `0`이므로 없어지고 뒤의 식만 남게 되고
    - `y`가 `1`일 때는 앞의 식만 남게 됨
  - 이러한` Cost Function`을 `BCE (binary cross entropy)` 라고 함
  - `BInary Classfication`을 풀 때는 `BCE` 사용

  ![image-20250203140707428](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203140707428.png)



- ##### Goal of ML Models

  ![image-20250203140909610](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203140909610.png)

  - `cost`와 `w`가 주어지면 `w`에 대한 `cost`가 주어지면 미분을 통해서 최저점을 찾을 수 있음