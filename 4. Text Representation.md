# 4. Text Representation

- ##### 이산 표현 ( Discrete Representation )

  - ##### 원-핫 인코딩 ( One - hot encoding )

    - 단어를 벡터로 표현하는 가장 간단한 방법
    - `단어 사전( dictionary )`을 구성하고 해당 단어를 `1`로, 그 밖의 단어는 `0`으로 표현

    ![image-20250205103817920](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205103817920.png)

  - ##### 원-핫 인코딩의 한계

  ![image-20250205104058272](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205104058272.png)
  
  - `개`랑 `늑대`가 달라서 `0`의 결과값을 가졌는데, `개`와 `잉어`의 결과값도 `0`임
  - 근데, 같은 `0`이더라도 사실 같은게 아닌데 결과값은 같아서 유사하다고 착각할 수 있음
  - 따라서, `원-핫 인코딩`의 한계는 유사성 비교가 불가능하다는 것임



- ##### 분산 표현 ( Distributed Representation )

  - `단어를 문맥에 기반`하여 표현하는 방법
  - 비슷한 문맥에서 등장하는 단어는 비슷한 의미를 가질 것이라는 가정에서 출발

  ![image-20250205111654462](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205111654462.png)

  - ##### 공기 행렬 ( Co-Occurrence Matrix )

  ![image-20250205113123565](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205113123565.png)

  - 위 그림에서 `행`단어 바로 다음에 `열`의 단어가 나오는지 카운트 하는 것
    - 예를 들어, 주어진 문장을 보면 `I`의 경우 `I`다음에 `I`가 온적이 없으니 `0`
    - `I` 뒤에 `like`는 2번을 나타났기 때문에 `2`
    - `enjoy`는 `I enjoy flying` 하나가 있기 때문에 `1` 
    - 이런식으로 카운트를 진행함 
  - 반대로 `열`기준으로 잡을 수도 있음
    - 예시로, `NLP`와 `like`를 볼 수 있음
    - 그림에는 `1`이라고 나와있는 이유는 `NLP`앞에 `like`가 있기 때문
    - `열`기준으로 보면 `like`뒤에 `NLP`가 있는 것을 찾는 것 이기 떄문에 `행렬`로 값을 나타내면 `1`의 결과값이 나옴



- ##### 코사인 유사도 ( Cosine Similarity )

  ![image-20250205131609396](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205131609396.png)

  - 두 벡터의 내적이란
    - `XㆍY = XY Cos(θ)` 두 벡터의 내적의 정의
    - 내적의 특징은 `두 벡터 요소의 구성이 비슷하면 큰 값을 가짐`
    - `X` 벡터와 `Y` 벡터의 요소들이 비슷한 위치에 값을 가지면 커지는게 `내적`
  - 내적만 측정해도 유사도로 쓸 수 있음
    - 그러나, 범위가 지정이 잘 안됨
  - 코사인의 특징
    - 두 벡터가 직각이면 `0`, 두 벡터가 다르면 직각
    - 두 벡터가 동일하면 `1`

  ![image-20250205131822156](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205131822156.png)

  - 위 그림은 두 벡터의 `내적`을 구하는 식



- ##### Problems with Co-Occurrence Vectors ( 공기 행렬의 문제 )

  ![image-20250205134040725](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205134040725.png)

  - 차원을 수를 줄이지 않으면 해결할 수 가 없다는 이야기가 있음

  - 그러기 위한 방법이 `특이값 분해 ( SVD : Singular Value Decomposition )`

  - `X`라는 매트릭스가 `n * m` 으로 구성되어 있다고 예시를 들면

    - `U`, `S`, `V` -> 3개의 매트릭스로 분해하는 것이 `SVD`
    - `U` = `n * r`, `S` = `r*r`, `V` = `r * m`

    ![image-20250205134450466](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205134450466.png)

    - 위 사진을 보면 왼쪽 대각선을 기준으로 아래는 `0`의 값만 가짐

    - 또한, `S`값들은 내림차순으로 정렬되어 있어서 `S1`부터 제일 큼(왼쪽 상단부터시작)

      ![image-20250205134616191](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205134616191.png)

      - 위 사진은 `r`보다 더 작게 한번 더 줄인거임

    - 따라서, 위의 그림들은

      ![image-20250205134847227](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205134847227.png)

      - 아래 그림과 같이 `m`차원을 `k`차원으로 바뀐 것 (차원을 축소한 것

  ![image-20250205160700427](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205160700427.png)

  - 차원을 거듭할 수록 좀 더 선명해지는 것을 볼 수 있음
  - 이 그림의 전반적인 공통적인 특징은 S1, S1다음의 공통적인 특징은  S2, 더 공통적인 특징은  S3
    - 전체를 포괄하는 것은 S1 (전체윤곽을 표현할 수 있는 것)
    - 점점 밑의 값을 곱함으로서 세부적인 디테일이 생김
    - 결론으론 `S`는 내림차순 정렬인데
    - S1은 가장 공통적인 부분, 즉, 포괄적인 것 많이 공통된 것임
    - 그래서 아래로 내려갈 수록 좀 더 디테일해진 부분들이 표현이 되는거임
    - 따라서, 아래의 값들을 곱할 수 록 좀 더 공통적인게 아닌 각 값들이 가진 특성이 추가되는 것이므로 사진이 선명해지는 것을 알 수 있음
  - 차원을 축소해서 공통 의미를 갖는 데까지만 남기는 것을 `SVD`

  ![image-20250205162000657](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205162000657.png)

  - 위의 그림에서 8차원을 2차원으로 변경할 거임

    - 그러면, `S1`, `S2`를 제외하고는 다 버림

  - ##### SVC의 문제점

    - 계산에 너무 오랜 시간이 소요됨
      - `n * m`행렬 계산 -> `O(mn²)`
    - 유연성이 너무 떨어짐
      - 새로운 단어나 문서가 추가될 경우에 `SVD`를 처음부터 다시 수행해야함
      - 치명적인 문제

  - ##### 해결방안

    ![image-20250205162444401](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205162444401.png)

  - ##### 분산 표현 ( Distiributed Representation ) 정리 / 다른말로는 Word Embedding

    - 고차원 one-hot 벡터를 가지고 있는 것을 해결하기 위해서 등장

      - 고차원의 벡터를 **저차원 실수 벡터로 바꾸는 방법**

    - 기본 아이디어 : **비슷한 문맥에서 나온 단어는 비슷한 뜻을 가짐** 으로써 출발을 함

    - 실수 float 로 표현하면서 표현력을 높이고 차원을 줄이는 것

    - ##### Word2Vec

      - CBOW

      ![image-20250205163554696](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205163554696.png)

      - **w(t)**가 예측되야할 현재 단어임
      - 현재 단어인 **w(t)**를 기준으로 뒤에서 2개 **w(t-1), w(t-2)**와 앞에서 2개인 **w(t+1), w(t+2)**를 바탕으로 해서 현재 단어를 예측하려고 하는 것이 **CBOW**

    - ##### Skip-Gram

      - **CBOW**의 반대 라고 보면 됨

      ![image-20250205163903490](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205163903490.png)

      - 현재 단어인 **w(t)**를 바탕으로 주변에 있는 단어를 예측하는 것이 **Skip-Gram**

    - **CBOW, Skip-Gram** 의 **입력 벡터, 출력 벡터**는 **one-hot**으로 표현됨

      - 우리가 학습하고자 하는 단어만큼의 **one-hot** 벡터로 표현
        - 100개면 100차원짜리의 **one-hot**벡터로 표현
        - 100차원중의 **w(t)**만 `1`이고 나머진 `0`임

      ![image-20250205164913217](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205164913217.png)

      - **Projection 레이어**와 **히든 레이어**의 차이점은 `시그모이드 함수`를 주는게 아님
        - 리니어하게 맵핑
        - **non linear function** 을 쓰지 않는 것
          - **ReLU**, **Sigmoid** 이런걸 사용하지 않음
      - 여기서 2개의 **Weight Vector**가 나오는데  **sum**을 기준으로 앞에 있는  **Weight Vector**와 **Sum** 뒤에 있는  **Weight Vector**가 있음
      - 이것들을 가져와서 **Word2Vec**을 사용
      - 보통은 앞쪽에 있는  **Weight Vector**을 사용
      - 이것을 거꾸로 적용한게 **Skip-Gram**

      ![image-20250205165601528](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205165601528.png)

      - 위의 그림은 **CBOW**를 활용하여 **context** 단어를 기준으로 **center word**를 **예측**한 것

      ![image-20250205165700255](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205165700255.png)

      - 위의 그림은 **Skip-Gram** 방법임

  - ##### 그림 요약

  ![image-20250205170043095](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205170043095.png)

  - **Cost Function**은 **Cross entropy**를 사용
  
  ![image-20250205171550187](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205171550187.png)
  
  ![image-20250206102117623](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206102117623.png)



- ##### Glove ( Global Vectors )

  - 말뭉치가 많다면 말뭉치에서 나오는 글로벌 정보 ( 확률 ) 값을 얻을 수 있음
  - 확률 값을 학습하여 벡터로 만듦

  ![image-20250206103035916](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206103035916.png)

  - **large**는 확률 값이 높은 것, 반대로 **small**은 낮은 것
  
  - **steam**과 **ice**를 비교했을 때 같은건 의미가 없으니 **large** or **small**값을 가진 것들만 구분을 지어주기 때문에 그것들을 학습에 반영함
  
    - 이렇게 학습에 하는 것이 **Glove**
  
  - **Log-bilinear model** : **w(i) * w(j) = logP(i|j)**  
  
    - 우리가 구하고자 하는 어떤 단어의 **벡터 내적**이 해당 **확률 값**이 되도록 가정하고 **벡터 내적**이 **확률 값**이 되도록 학습
  
  - **Vector differences** 
  
    - 찾고자 하는 단어와 **주어진 단어들의 차이**의 내적
    - 실제 수식은 **WxWa - WxWb = Wx(wa - wb)** 
      - 여기서 **WxWa = logP(x|a)**. **WxWb = logP(x|b)**
      - 따라서 그림과 같은 **log** 수식이 완성됨
  
    ![image-20250206104344789](C:/Users/SeongBae/AppData/Roaming/Typora/typora-user-images/image-20250206104344789.png)
  
  - ##### Object Function of Glove
  
    -  두 개의 내적이 확률 값이 되도록 만들고 **함수**를 하나 만들어 곱함
  
  ![image-20250206110303487](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206110303487.png)
  
  - **f(x)**의 값을 여기선 **100**을 기준으로 그 이상은 `1`로 수렴되고 그 나머지는 그래프와 같이 표현되게 만듦
  
  - 위의 식과 같이 **목적 학습**을 만들고 이것을 학습에 활용
  - 결국 **Glove**는 **Co-occurrence probability** 가까워지도록 내적 값을 학습해서 **워드 벡터**를 만들어 내는 것



- **From One-hot Rep. to Distributed Rep.**

  - 어떤 분야에서 오타가 많거나 철자 오류가 많을 경우는 **Word**단위로 할 수 없음

    -  이럴 경우에는 **부분 단어( Subword )**로 처리해야 하는 경우가 있음
      - 예를 들어, **건국대학교** 라는 단어가 있으면 `건국` `국대` `대학` 등등 이런 것을 **부분** **단어**라고 함
    - 오타가 많아서 사전에 없는 경우가 있음
      - 이러한 경우를 **UNK ( Unknown word )** or **OOV ( out of vocabulary )**
      - 예를 들어, **건국다학교** 라고 오타가 발생했음
      - 이런 경우, **fastText**를 이용하면 됨
        - **fastText** 란 **부분 단어**로 학습하는 것, 노이즈에 강함
      - 따라서, **건국다학교**라고 입력을 해도 **건국**, **학교**라는 **부분 단어**를 도출할 수 있기 때문에 단어의 일부가 매핑이 됨

  - **Glove**

    - 동시 등장 확률을 함께 학습
    - 성능이 좋은 것을 쓰고 싶을 때 사용

  - ##### fastText

    - 노이즈가 많을 때 사용

  ![image-20250206114508724](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206114508724.png)



- ##### Add Contexts

  - **Word2Vec**와 **Glove**의 **단점**

    - 특정 단어가 하나의 벡터로 표현됨
    - 예를 들어 , **배** -> 여러가지의 의미를 가지고 있음
    - 앞의 두개의 방법을 통해 학습하면 어디에도 속하지 않고 애매하게 위치하고 있음
    - 해당 **배**가 과일인지, 신체 기관인지, 탈 것인지 알 수가 없음
    - 따라서, 주변의 문장을 보고 조정해야함
      - 그 방법이 아래의 방법들

  - ##### Cove

    - **MT-LSTM**을 통한 사전학습 **h = MT-LSTM(Glove(w))**
      - 문장을 넣어서 학습
    - 만들어놓은 기계 학습에 **Glove**을 넣어 재학습시키는 것
    - 문장을 학습시켜놓고 **Glove**와 같은 단어를 과일의 `배`인지, 탈것의 `배`인지, 신체의 `배`인지 가정을 하고 학습을 시킴
      - 각각 하나의 `배`가 아닌 각각 다른 의미를 가진 `배`로 벡터값이 저장됨
      - 여기서 이러한 벡터값을 가져다 쓰는 것이 **Cove**

  - ##### ELMo

    - **Language Model**을 통한 사전학습
    - **Cove**와 달리 한국어, 영어 쌍으로 준비할 필요가 없음
    - 한국어를 기준으로 **ELMo** 모델을 만들고 싶으면 한국어만 있으면 됨
      - 문장도 품사나 구문 부사가 된 문장이 아닌 그냥 **순수한 문장(원시 말뭉치, raw corpas)**
        - 아무 문장이나 크롤링 데이터를 통해 학습
    - **Language Model** 도입해서 입력된 문장에 있는 각 단어의 문맥을 반영한 벡터값을 뽑아내기 위한 모델 
    - 대용량의 원시 말뭉치는 가지고와서 입력을 하는데 입력되는 값들은 `word2vec, glove` 고정된 벡터값을 가져와서
      **bi-lstm**에 통과를 시킴
      - 주위에 있는 단어들의 값을 보고 현재 단어의 벡터를 업데이트 하겠다는 것 (문맥을 보고 이해)
      - **bi-lstm** 이니까 정방향으로 반영하고 역방향으로도 반영함
      - 모두 통과시키면 문맥이 반영된 벡터값이 들어감
    - 그 이후 한번 더 입력을 시키는데 **원래 word2vec, glove에 있는 값**이랑 **문맥이 반영된 벡터값**을 합쳐서 다시 한번 입력
      - 이러한 과정을 **Residual connection**
        - 이렇게 하는 이유는 원래 의미를 상기시키기 위해
        - 한쪽만 반영해서 다른 한쪽의 의미를 잃어버릴 수 있기 때문
    - 여기서 만약에 **happy** 라는 벡터를 뽑는 다 하면
      - **원래의 happy,  처음 BiLSTM하고 난뒤에 happy 값, 두 번째 BiLSTM하고 난 뒤에 happy값**을 적당히 Weigthed sum을  하고 **loss function**을 줘서 학습시킴
      - 그걸 다 합친 값을 문맥이 반영된 **happy**의 벡터값 이라함
    
    ![image-20250206135102296](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206135102296.png)
    
    - **SOAT ( state of the art )**
      - 세계(현재) 최고 성능
    
    ![image-20250206135419113](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206135419113.png)
    
    - **ELMo**를 반영한 뒤에 성능이 올랐음
    - 이러한 결과를 보고 우리가 계산한 **Word2vec, Glove**의 벡터값을 그냥 쓰는 것보단 문장이 존재하면 그 문장의 주위 문맥을 가지고 보다 정확한 **Distributed Representation ( 분산 표현, 단어를 문맥에 기반하여 표현하는 방법 )** 를 이용하는 게 너무나 효과적임
    - 만약에, 우리가 문장 단위 입력을 통해 모델을 만들면 최소한 **ELMo**정도는 써야됨
    - 고정된 **Distributed Representation** 쓰고싶으면 **Glove** 사용
    - 주위 문맥이 존재하는 **task** 에서는 **ELMo** 사용

- ##### Language Model Using RNN

  - **Language Model** 

    - 어떤 단어 다음에 어떤 단어가 나오는 것이 그 언어에서 가장 그럴듯 한지 계산하는 확률 모델

  - 그림으로 예를 들면

    - **tom**이라는 나올 확률 * **TOM**이 나온 시점에서 **likes**가 나올 확률 * **tom, likes**가 나온 시점에서 **beer**가 나올 확률 * **tom, likes, beer**가 나온 시점에서 **문장이 끝날** 확률

    ![image-20250206131355195](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206131355195.png)

    - **tom likes beer** 라는 문장이 나올 확률이 얼마냐 라는 것이 **language model**

    - 얼마나 올바른 문맥인지 계산하는 것

      ![image-20250206132001319](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206132001319.png)

      - 사진에서의 **softmax**에는 언어가 가지고 있는 `vocabulary`  2~3만개의 데이터셋을 넣어놈
      - **RNN(Recurrent Neural Network)**
        - 시계열 또는 순차 데이터를 예측하는 딥러닝을 위한 신경망 아키텍처



- ##### 실습 : Word2Vec 사용

![image-20250206142247435](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206142247435.png)

![image-20250206153553083](C:/Users/SeongBae/AppData/Roaming/Typora/typora-user-images/image-20250206153553083.png)

- 해당 그림의 설명을 추가하면

  - 여기서 min_count - > 최소 N번 이상 나온 것만 학습을 참여시키 겠다는 것
  - `context-window` 크기란 ?
    - 앞 뒤로, n개의 단어를 비교

  ![image-20250206153835316](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206153835316.png)

- ##### Glove 실습

  ![image-20250206154103846](C:/Users/SeongBae/AppData/Roaming/Typora/typora-user-images/image-20250206154103846.png)

  - **corpus.fit** 
    - ``Co-Occurrence Martix``에 값을 채우는 함수
      - 여기서 `result` = 앞에서 명사만 뽑았던 벡터
    - `no_components` 
      - 앞에서 세팅한 차원의 수 입력
    - `epochs`
      - 전체 훈련 데이터셋을 한 번 완전히 모델에 학습시키는 과정



- ##### 실습 : Embedding Layer

  ![image-20250206155149511](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206155149511.png)

  - 단어들을 벡터화 한 것 = 임베딩 벡터

  - ##### fine-tuning ( 파인 튜닝 )

    - 사전 학습된 벡터 또는 임의의 벡터를 만들고 학습 과정에서 업데이트

  - ##### lookup Table

    - 임베딩 벡터를 담고 있는 `Table`

  - 임베딩 레이어에 업데이트 하고 싶은 단어를 배정해주면 **lookup Tabled**에 업데이트 됨

  ![image-20250206160244021](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206160244021.png)

  - **unk**
    - `unknown`으로 향후 모르는 단어가 나올 때 이 태그가 붙음
  - **pad**
    - `padding`으로. 기사 별 길이를 맞춰주기 위해 붙을 태그

  ![image-20250206161259150](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206161259150.png)

  - ##### num_embeddings

    -  임베딩 테이블의 사이즈

  - ##### embedding_din

    - 차원의 수

  - ##### padding_idx

    - 패딩과 관련된 인덱스

  - 위 처럼 임베딩 레이어를 만들면 위의 코드처럼 우리가 만들었던 lookup talbe을 업데이트 하면서 그 값을 파인튜닝을 하게 됨



- ##### Word Piece for any Language

  - 워드 인베딩의 값을 얻으려면 ?
    - 문장이 입력되었을 때 그것으로부터 단어를 찾아내야하고 단어를 임베딩 값으로 표현
  - 문장으로부터 단어를 뽑아내는 프로그램 -> **형태소 분석기**
  - 영어는 공백 단위로 짜르면 하나하나 단어가 됨
    - 형태소 분석기를 만들기 되게 쉬움
  - 하지만, 한국어는 아님
    - 공백 단위로 자르면 **어절** 이라고 하는데 한국어는 하나의 단어가 되지 않음
  - 모든 언어에 임베딩을 뽑아낼 수 있는 방법을 고안해서 나온 것이 **Word Piece** 다른 말로 **Byte Pair Encoding**
  - **음절**
    - 우리나라 기준
    - 예시 ) 워드피스
      - `워`, `드`, `피`, `스` 이렇게 하나하나씩 쪼갬
    - 영어 기준
    - 예시 ) WordPiece
      - `w`, `o`, `r`....
      - 알파벳 기준으로 쪼갬

  ![image-20250206162635893](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206162635893.png)

  - 위 그림 처럼 영어 알파벳 기준으로 처음 쪼개서 만든 것이 기본 `Vocabulary`

  - 그 다음에는`Spacing Units`에 나온 단어를 기준으로 알파벳을 `2개씩` 묶음

    - ex) `lo`, `ow`, `we`, `er` ...

    ![image-20250206163002405](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206163002405.png)

    - 위의 그림과 같아짐

    - 그 이후 각각 카운팅

      - 예시를 들면, 처음의 `lo`, `ow`는 `low`에서 파생된 건데 5번씩 카운팅을 해줘야함
      - 아래 `lo` ~ `er`까지는 `lower`가 `2`이니까 카운트가 2임
      - 위 처럼 진행한 후에 겹치는 단어끼리 더하면 되는데
      - `lo`는 최종적으로 `7`이 되는거임

    - 이런식으로 카운팅하고 가장 **빈도( frequency )**가 많은 단어를 하나 선택

      - 위의 예시론 `es`가 9번이라 제일 많아서

      ![image-20250206163706922](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206163706922.png)

      - 기존의 `vocabulary`에 추가
      - 만약에 빈도가 겹치는 것이 많으면 랜덤으로 임의선정

    ![image-20250206164306570](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206164306570.png)

    - 그러면 또 다시 `2개`씩 묶는 다고 했을 때, 새로 추가된 `es`는 하나의 단어로 취급하여 묶음
      - 그림에 적힌 것처럼 `wes`, `est`, `des`, `est` 이렇게 묶어버림
      - 그 후 또 가장 많이 나타난 단어를 묶음
        - 그러면 `est` = `6`개와 `3`개를 가짐
    - 이후에 또 `est`를 `vocabulary`에 추가

    ![image-20250206165059655](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206165059655.png)

    - 이런식을 반복해서 `vocabulary`를 업데이트
      - 계속해서 반복하는 데 우리가 타겟한 `vocabulary`수가 나올 때 까지 반복
    - `vocabulary`가 완성되면 해당 `vocabulary`를 바탕으로 새로운 단어가 입력되었을 때
      가장 길게 짜를 수 있는 방법대로 짜르면 됨
      - 그러면 그 짤라진 것을 `단어`라고 취급을 함
      - 이 것을 `Word Piece` 라고 정의



- ##### 확인 문제 : Byte Pari Encoding

  ![image-20250206170218449](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250206170218449.png)

  - 가장 긴 걸로 커팅해서
    - `low` , `e`, `r`, `wid`, `est` 로 자르고 이 각각을 단어로 인정
    - 이러한 과정을 `Word Piece`
      - 가장 긴 거 부터 커팅하는 방법
      - 실제 단어는 아니고 `subword`임
        - 대신 의미가 있는 단위로 자를 수 있음
