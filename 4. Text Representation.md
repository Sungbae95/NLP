# 4. Text Representation

- ##### 이산 표현 ( Discrete Representation )

  - ##### 원-핫 인코딩 ( One - hot encoding )

    - 단어를 벡터로 표현하는 가장 간닪난 방법
    - `단어 사전( dictionary )`을 구성하고 해당 단어를 `1`로, 그 밖의 단어는 `0`으로 표현

    ![image-20250205103817920](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205103817920.png)

  - ##### 원-핫 인코딩의 한계

  ![image-20250205104058272](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205104058272.png)
  
  - `개`랑 `늑대`가 달라서 `0`의 결과값을 가졌는데, `개`와 `잉어`의 결과값도 `0`임
  - 근데, 같은 `0`이더라도 사실 같은게 아닌데 결과값은 같아서 유사하다고 착각할 수 있음
  - 따라서, `원-핫 인코딩`의 한계는 유사성 비교가 불가능하다는 것임



- ##### 분산 표현 ( Distiributed Representation )

  - `단어를 문맥에 기반`하여 표현하는 방법
  - 비슷한 문맥에서 등장하는 단어는 비슷한 의미를 가질 것이라는 가정에서 출발

  ![image-20250205111654462](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205111654462.png)

  - ##### 공기 행렬 ( Co-Occurrence Matrix )

  ![image-20250205113123565](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205113123565.png)

  - 위 그림에서 `행(왼쪽 기준으로 수직으로 내려가는 단어)단어` 바로 다음에 `열(수평단어)`의 단어가 나오는지 카운트 하는 것
    - 예를 들어, 주어진 문장을 보면 `I`의 경우 `I`다음에 `I`가 온적이 없으니 `0`
    - `I` 뒤에 `like`는 2번을 나타났기 때문에 `2`
    - `enjoy`는 `I enjoy flying` 하나가 있기 때문에 `1` 
    - 이런식으로 카운트를 진행함 
  - 반대로 `열`기준으로 잡을 수도 있음
    - 예시로, `NLP`와 `like`를 볼 수 있음
    - 그림에는 `1`이라고 나와있는 이유는 `NLP`앞에 `like`가 있기 때문
    - `열`기준으로 보면 `like`뒤에 `NLP`가 있는 것을 찾는 것 이기 떄문에 `행렬`로 값을 나타내면 `1`의 결과값이 나옴



- ##### 코사인 유사도 ( Cosine Similarity )

  ![image-20250205131609396](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205131609396.png)

  - 두 벡터의 내적이란
    - `XㆍY = XY Cos(θ)` 두 벡터의 내적의 정의
    - 내적의 특징은 `두 벡터 요소의 구성이 비슷하면 큰 값을 가짐`
    - `X` 벡터와 `Y` 벡터의 요소들이 비슷한 위치에 값을 가지면 커지는게 `내적`
  - 내적만 측정해도 유사도로 쓸 수 있음
    - 그러나, 범위가 지정이 잘 안됨
  - 코사인의 특징
    - 두 벡터가 직각이면 `0`, 두 벡터가 다르면 직각
    - 두 벡터가 동일하면 `1`

  ![image-20250205131822156](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205131822156.png)

  - 위 그림은 두 벡터의 `내적`을 구하는 식



- ##### Problems with Co-Occurrence Vectors ( 공기 행렬의 문제 )

  ![image-20250205134040725](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205134040725.png)

  - 차원을 수를 줄이지 않으면 해결할 수 가 없다는 이야기가 있음

  - 그러기 위한 방법이 `특이값 분해 ( SVD : Singular Value Decomposition )`

  - `X`라는 매트릭스가 `n * m` 으로 구성되어 있다고 예시를 들면

    - `U`, `S`, `V` -> 3개의 매트릭스로 분해하는 것이 `SVD`
    - `U` = `n * r`, `S` = `r*r`, `V` = `r * m`

    ![image-20250205134450466](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205134450466.png)

    - 위 사진을 보면 왼쪽 대각선을 기준으로 아래는 `0`의 값만 가짐

    - 또한, `S`값들은 내림차순으로 정렬되어 있어서 `S1`부터 제일 큼(왼쪽 상단부터시작)

      ![image-20250205134616191](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205134616191.png)

      - 위 사진은 `r`보다 더 작게 한번 더 줄인거임

    - 따라서, 위의 그림들은

      ![image-20250205134847227](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205134847227.png)

      - 아래 그림과 같이 `m`차원을 `k`차원으로 바뀐 것 (차원을 축소한 것

  ![image-20250205160700427](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205160700427.png)

  - 차원을 거듭할 수록 좀 더 선명해지는 것을 볼 수 있음
  - 이 그림의 전반적인 공통적인 특징은 S1, S1다음의 공통적인 특징은  S2, 더 공통적인 특징은  S3
    - 전체를 포괄하는 것은 S1 (전체윤곽을 표현할 수 있는 것)
    - 점점 밑의 값을 곱함으로서 세부적인 디테일이 생김
    - 결론으론 `S`는 내림차순 정렬인데
    - S1은 가장 공통적인 부분, 즉, 포괄적인 것 많이 공통된 것임
    - 그래서 아래로 내려갈 수록 좀 더 디테일해진 부분들이 표현이 되는거임
    - 따라서, 아래의 값들을 곱할 수 록 좀 더 공통적인게 아닌 각 값들이 가진 특성이 추가되는 것이므로 사진이 선명해지는 것을 알 수 있음
  - 차원을 축소해서 공통 의미를 갖는 데까지만 남기는 것을 `SVD`

  ![image-20250205162000657](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205162000657.png)

  - 위의 그림에서 8차원을 2차원으로 변경할 거임

    - 그러면, `S1`, `S2`를 제외하고는 다 버림

  - ##### SVC의 문제점

    - 계산에 너무 오랜 시간이 소요됨
      - `n * m`행렬 계산 -> `O(mn²)`
    - 유연성이 너무 떨어짐
      - 새로운 단어나 문서가 추가될 경우에 `SVD`를 처음부터 다시 수행해야함
      - 치명적인 문제

  - ##### 해결방안

    ![image-20250205162444401](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205162444401.png)

  - ##### 분산 표현 ( Distiributed Representation ) 정리 / 다른말로는 Word Embedding

    - 고차원 one-hot 벡터를 가지고 있는 것을 해결하기 위해서 등장

      - 고차원의 벡터를 **저차원 실수 벡터로 바꾸는 방법**

    - 기본 아이디어 : **비슷한 문맥에서 나온 단어는 비슷한 뜻을 가짐** 으로써 출발을 함

    - 실수 float 로 표현하면서 표현력을 높이고 차원을 줄이는 것

    - ##### Word2Vec

      - CBOW

      ![image-20250205163554696](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205163554696.png)

      - **w(t)**가 예측되야할 현재 단어임
      - 현재 단어인 **w(t)**를 기준으로 뒤에서 2개 **w(t-1), w(t-2)**와 앞에서 2개인 **w(t+1), w(t+2)**를 바탕으로 해서 현재 단어를 예측하려고 하는 것이 **CBOW**

    - ##### Skip-Gram

      - **CBOW**의 반대 라고 보면 됨

      ![image-20250205163903490](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205163903490.png)

      - 현재 단어인 **w(t)**를 바탕으로 주변에 있는 단어를 예측하는 것이 **Skip-Gram**

    - **CBOW, Skip-Gram** 의 **입력 벡터, 출력 벡터**는 **one-hot**으로 표현됨

      - 우리가 학습하고자 하는 단어만큼의 **one-hot** 벡터로 표현
        - 100개면 100차원짜리의 **one-hot**벡터로 표현
        - 100차원중의 **w(t)**만 `1`이고 나머진 `0`임

      ![image-20250205164913217](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205164913217.png)

      - **Projection 레이어**와 **히든 레이어**의 차이점은 `시그모이드 함수`를 주는게 아님
        - 리니어하게 맵핑
        - **non linear function** 을 쓰지 않는 것
          - **ReLU**, **Sigmoid** 이런걸 사용하지 않음
      - 여기서 2개의 **Weight Vector**가 나오는데  **sum**을 기준으로 앞에 있는  **Weight Vector**와 **Sum** 뒤에 있는  **Weight Vector**가 있음
      - 이것들을 가져와서 **Word2Vec**을 사용
      - 보통은 앞쪽에 있는  **Weight Vector**을 사용
      - 이것을 거꾸로 적용한게 **Skip-Gram**

      ![image-20250205165601528](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205165601528.png)

      - 위의 그림은 **CBOW**를 활용하여 **context** 단어를 기준으로 **center word**를 **예측**한 것

      ![image-20250205165700255](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205165700255.png)

      - 위의 그림은 **Skip-Gram** 방법임

  - ##### 그림 요약

  ![image-20250205170043095](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205170043095.png)

  - **Cost Function**은 **Cross entropy**를 사용
  
  ![image-20250205171550187](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250205171550187.png)
  
  

