# 2. Artificial Neural Network: PART 1

- ##### ANN (Artificial Neural Network)

  - 수학적 논리학이 아닌 인간의 두뇌를 모방하여 수많은 간단한 처리기들(뉴런)의 네트워크를 통해 문제를 해결하는 기계학습 모델

  ![image-20250203150330442](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203150330442.png)

  ![image-20250203150557615](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203150557615.png)

  - 위와 같이 모방한 것이 `ANN`
    - `weight`를 업데이트 하는 것을 **델타룰**
      - 정답과 출력을 비교하여 그 차이를 가중치에 조정하는 것
      - 인공지능의 학습법
  
  
  ![image-20250203151112463](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203151112463.png)



- ##### 퍼셉트론 (Perceptron)

  ![image-20250203152634016](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203152634016.png)

  - 해당 그림의 `output`에서 다음 층을 쌓으면 `Multi layer`가 됨
  - 위 그림은 `Single-layer-perceptron`
  - 하나의 `perceptron`은 하나의 `Binominal Classifier`임
    - 하나의 선으로 양쪽을 구분하는 `Classifier`라고 생각
    - 단점은 `XOR`문제를 못품



- ##### Multinomial Classification

  ![image-20250207151613941](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250207151613941.png)

  - 직선 하나를 그어놓고 해당 값이 맞는 지 아닌지 구분하는 것
  
  - 위 그래프를 보면 각 `직선`을 그어놓고 `A`와 `B`인지 애매모호 한 값이 나올 때가 있음
    - 그럴 때는 더 확실히 경계값의 들어와있는지 아닌지로 비교함
    - 직선과 더 가까운 것, 혹은 직선과 더 먼 것으로 비교(기준점을 정해서)
    - 만약에 어느 한 점이 `Not A, B, C`일 경우에는 각 값들을 비교해서 선택해야함
      - 가장 `not ?` 인것으로 선정하면 됨
      - ex) a가 20% 부정, b가 30% 부정, c가 40% 부정일 경우에는 부정의 값에서 제일 작은 값을 선택해야함
      - 다라서, `a`가 선택되는 것을 볼 수 있음
  
  ![image-20250203155159103](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203155159103.png)
  
  ![image-20250203160304311](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203160304311.png)
  
  - 각각 하나일 경우에는 `BCE`를 사용하면 되는데 위의 그림은 3개임
  - 이럴 때는 `Cross Entropy`를 사용
    - 이것은 `Entropy`값들의 합임
    - `L` = 정답, `S` 각각의 `Entropy`
  - `Multinomial Classification`에서는 `Cross Entropy`라는 `Cost Function`을 사용함
  - 위 그림에 나와 있는 ⊙ 해당 기호는 `곱셈 (x)`을 의미함
  - `Cross Entropy`는 결국에는 우리가 출력한 분포랑 정답 분포랑 얼마나 유사한지 계산하는 것



- ##### SoftMax

  ![image-20250207152453193](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250207152453193.png)
  
  - `Sigmoid` 함수에 의해서 각각 0~1 사이 값을 가진 `y`의 값들은 읽기가 어려움
    - 또한, 각각 얼마나 크고 작은지 비교가 어려움
  - 따라서, 확률처럼 보이게 바꾸려고 함
    - 어떻게 ? `y`값들을 합치면 `1`이 되도록
    - 이 과정을 우리는 `SoftMax`라고 함
      - 확률로 표현하는 것
  - 해당 과정은 `y1 + y2 + yn...`으로 더한 값을 나누어도 됨
    - 하지만, 이렇게하면 작은 값들의 영향, 큰 값들의 영향 각각 영향 받는게 다름
    - 따라서, 평탄화 하는 방법을 사용함
  
  ![image-20250203162340639](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203162340639.png)
  
  - 위의 방법으로 평탄화 과정을 진행함
  - `Softmax`를 진행한 후의 `y`의 값을 `Probabilities`라 함
  - 반대로 진행하기 전의 값은 `Scores`라고 표현
  - `SoftMax` 진행 후 가장 큰 값을 출력함
    - 이것을 `argmax`라고 함
  
  - 위 그림을 예시로
  - `SoftMax`를 하기 전의 `y`의 값들은 `Scores`이며 `0`과`1`의 사이를 가지게 됨
  - `SoftMax`를 진행 후 `Probabilities`라는 값을 가지게 되며 모든 `y`의 값들을 합치면 `1`이 되는 소숫점 숫자들로 구성이 됨
  - 그 중에서 가장 큰 값을 출력하게 되며 그것은 `argmax`이며 `1`이라고 표현됨



- ##### Non-linear Problems (비선형 분리 문제)

  ![image-20250203163032313](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203163032313.png)

  - `Multinomial`도 여전히 `xor`과 같은 비선형 분리 문제는 풀지 못했음
  - SVM에서 커널이 하는 일은 고차원 공간으로 사상시키는 것
  - 실제로는 `cost`비용이 많이 들어서 `kernel trick`을 이용하여 같은 차원에서 다른 곳으로 옮겨 버림
  - 옮기고 매핑해서 그 다음에 직선의 선형 분리 문제로 바꾼 다음에 해결했음
  - 위와 같은 방법을 사용하여 `Single-layer perceptron (선을 찾는 문제)`을 여러 개 쌓으면 그 선에서 있던 문제들의 위치들을 다른 공간으로 옮겨버림(커널 역할)
  - 옮긴 후에 선형 분리 문제로 해결 -> 이것이 `Multi-layer perceptron`

- `Multi-layer perceptron`

  ![image-20250203165436474](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203165436474.png)

  - 4개의 점이 3개의 점으로 mapping 되면서 다른 위치로 옮겨졌고 `SVM`에서 이러한 방법을이 **kernel trick**
  - 위의 그림의 예시를 보면 3개의 점이 되면서 `선형 문제 분리`로 바뀐 것을 알 수 있음
  - 위의 그림처럼 구조를 만들면 `XOR`문제가 해결이 가능함