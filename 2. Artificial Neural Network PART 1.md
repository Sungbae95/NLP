# 2. Artificial Neural Network: PART 1

- ##### ANN (Artificial Neural Network)

  - 수학적 논리학이 아닌 인간의 두뇌를 모방하여 수많은 간단한 처리기들(뉴런)의 네트워크를 통해 문제를 해결하는 기계학습 모델

  ![image-20250203150330442](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203150330442.png?token=AXLXXIRHGLX3DGCLV6TJZFTHULD2M)

  ![image-20250203150557615](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203150557615.png?token=AXLXXIWVRBITKMZLPNYAO6THULD2Q)

  - 위와 같이 모방한 것이 `ANN`

  ![image-20250203151112463](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203151112463.png?token=AXLXXIXHDA3VA6GUAXFAVJTHULD3A)



- ##### 퍼셉트론 (Perceptron)

  ![image-20250203152634016](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203152634016.png?token=AXLXXISD2OD2WK7M5ZAURBTHULD3W)

  - `구조`에서 `output`을 이용해여 다음 층을 쌓으면 `Multi layer`가 됨
  - 옆으로 펼쳐 놓으면 `Single-layer-perceptron`
  - 하나의 `perceptron`은 하나의 `Binominal Classifier`임
    - 하나의 선으로 양쪽을 구분하는 `Classifier`라고 생각
    - 단점은 `XOR`문제를 못품



- ##### Multinomial Classification

  ![image-20250203154003782](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203154003782.png?token=AXLXXIR5OLK5RV4JF65R74THULD4E)

  - 위 그래프를 보면 각 `직선`을 그어놓고 `A`와 `B`인지 애매모호 한 값이 나올 때가 있음
    - 그럴 때는 더 확실히 경계값의 들어와있는지 아닌지로 비교함
    - 만약에 어느 한 점이 `Not A, B, C`일 경우에는 가장 아닌 게 아닌 거 즉, 각 값들을 비교해서 선택해야함

  ![image-20250203155159103](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203155159103.png?token=AXLXXIW4TRKZ5KXXO3B5IOLHULD4W)

  ![ ](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203160304311.png?token=AXLXXISMK7AFJORIA6GSNHTHULD44)

  - 각각 하나일 경우에는 `BCE`를 사용하면 되는데 위의 그림은 3개임
  - 이럴 때는 `Cross Entropy`를 사용
    - 이것은 `Entropy`값들의 합임
    - `L` = 정답, `S` 각각의 `Entropy`
  - `Multinomial Classification`에서는 `Cross Entropy`라는 `Cost Function`을 사용함
  - 위 그림에 나와 있는 ⊙ 해당 기호는 `곱셈 (x)`을 의미함
  - `Cross Entropy`는 결국에는 우리가 출력한 분포랑 정답 분포랑 얼마나 유사한지 계산하는 것



- ##### SoftMax

  -  `Sigmoid` 함수에 의해서 각각 0~1 사이 값을 가진 `y`의 값들은 읽기가 어려움
  - 각각 얼마나 크고 작은지 비교가 어려움
  - 따라서, 확률처럼 보이게 바꾸려고 함
    - 어떻게 ? `y`값들을 합치면 `1`이 되도록
    - 이 과정을 우리는 `SoftMax`라고 함
  - 해당 과정은 `y1 + y2 + yn...`으로 값을 나누어도 됨
    - 하지민, 이렇게하면 작은 값들의 영향, 큰 값들의 영향 각각 영향 받는게 다름
    - 따라서, 평탄화 하는 방법을 사용함

  ![image-20250203162340639](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203162340639.png?token=AXLXXIXM562KIAFD47PLDE3HULD5Q)

  - 위의 방법으로 평탄화 과정을 진행함
  - `Softmax`를 진행한 후의 `y`의 값을 `Probabilities`라 함
  - 반대로 진행하기 전의 값은 `Scores`라고 표현
  - `SoftMax` 진행 후 가장 큰 값을 출력함
    - 이것을 `argmax`라고 함

  ![image-20250203162742618](C:\Users\SeongBae\Desktop\자연어처리 강의\image-20250203162742618.png)

  - 위 그림을 예시로
  - `SoftMax`를 하기 전의 `y`의 값들은 `Scores`이며 `0`과`1`의 사이를 가지게 됨
  - `SoftMax`를 진행 후 `Probabilities`라는 값을 가지게 되며 모든 `y`의 값들을 합치면 `1`이 되는 소숫점 숫자들로 구성이 됨
  - 그 중에서 가장 큰 값을 출력하게 되며 그것은 `argmax`이며 `1`이라고 표현됨



- ##### Non-linear Problems (비선형 분리 문제)

  ![image-20250203163032313](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203163032313.png?token=AXLXXIUGQTMSIY5KBNYGVHLHULD56)

  - SVM에서 커널이 하는 일은 고차원 공간으로 사상시키는 것
  - 실제로는 `cost`비용이 많이 들어서 `kernel tric`을 이용하여 같은 차원에서 다른 곳으로 옮겨 버림
  - 옮기고 매핑해서 그 다음에 직선의 선형 분리 문제로 바꾼 다음에 해결했음
  - 위와 같은 방법을 사용하여 `Single-layer perceptron (선을 찾는 문제)`을 여러 개 쌓으면 그 선에서 있던 문제들의 위치들을 다른 공간으로 옮겨버림(커널 역할)
  - 옮긴 후에 선형 분리 문제로 해결 -> 이것이 `Multi-layer perceptron`

- `Multi-layer perceptron`

  ![image-20250203165436474](https://raw.githubusercontent.com/Sungbae95/NLP/main/image/image-20250203165436474.png?token=AXLXXIUCSB3X3ZAA3WZSE63HULD6E)

  